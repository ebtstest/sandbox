import numpy as np
import pandas as pd
from pandas import DataFrame, Series
import matplotlib.pyplot as plt

s = Series([1, 2, 3, 4])
s
            Out [2]:
            0 1
            1 2
            2 3
            3 4

s[[1, 3]]         # return a Series with the rows with index 1 and 3
            Out [3]:
            1 2
            3 4

dates = pd.date_range('2014-07-01', '2014-07-06')

temps1 = Series([80, 82, 85, 90, 83, 87], index = dates)

temps1.mean()

emps2 = Series([70, 75, 69, 83, 79, 77],  index = dates)
# the following aligns the two by their index values
# and calculates the difference at those matching labels
temp_diffs = temps1 - temps2

temps_df = DataFrame({'Missoula': temps1, 'Philadelphia': temps2})
temps_df

                        Missoula Philadelphia
                2014-07-01    80 70
                2014-07-02    82 75
                2014-07-03    85 69
                2014-07-04    90 83
                2014-07-05    83 79
                2014-07-06    87 77

# return both columns in a different order                
temps_df[['Philadelphia', 'Missoula']]

# retrieve the Missoula column through property syntax
temps_df.Missoula

# add a column to temp_df that contains the difference in temps
temps_df['Difference'] = temp_diffs
temps_df
                            Missoula Philadelphia Difference
                2014-07-01       80 70 10
                2014-07-02       82 75 7
                2014-07-03       85 69 16
                2014-07-04       90 83 7
                2014-07-05       83 79 4
                2014-07-06       87 77 10

# get the columns, which is also an Index object
temps_df.columns
    Index([u'Missoula', u'Philadelphia', u'Difference'], dtype='object')
    
# get the row at array position 1
temps_df.iloc[1]
                Missoula 82
                Philadelphia 75
                Difference  7

# the names of the columns have become the index they have been 'pivoted'
temps_df.ix[1].index
      Index([u'Missoula', u'Philadelphia', u'Difference'], dtype='object')

# retrieve row by index label using .loc
temps_df.loc['2014-07-03']
                Missoula 85
                Philadelphia 69
                Difference 16


# read the data and tell pandas the date column should be
# a date in the resulting DataFrame
df = pd.read_csv('data/test1.csv', parse_dates=['date'])


# read in again, now specify the data column as being the
# index of the resulting DataFrame
df = pd.read_csv('data/test1.csv', parse_dates=['date'], index_col='date')

# imports for reading data from http://finance.yahoo.com
from pandas.io.data import DataReader
from datetime import date
from dateutil.relativedelta import relativedelta
# read the last three months of data for GOOG
goog = DataReader("GOOG", "yahoo", date.today() + relativedelta(months=-3))


#Get Stock price quote from Pandas.io.data
#
import pandas.io.data as web
start = datetime.datetime(2010,1,1)
end = datetime.datetime(2016,5,1)
df = web.DataReader("TSLA", "yahoo", start, end)


# plot the Adj Close values we just read in
goog.plot(y='Adj Close');


# lets do this with a numpy array
import numpy as np
array_to_square = np.arange(0, 100000)
array_to_square ** 2

a1 = np.array([1, 2, 3, 4, 5])
np.size(a1)

# shorthand to repeat a sequence 10 times
a3 = np.array([0]*10)
      array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])


# create a numpy array of 10 0.0's
np.zeros(10)
      array([ 0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])
      
      
# force it to be of int instead of float64
np.zeros(10, dtype=int)
      array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
      
      
# make "a range" starting at 0 and with 10 values
np.arange(0, 10)
      array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
      

# 0 <= x < 10 increment by two
np.arange(0, 10, 2)
      array([0, 2, 4, 6, 8])


# 10 >= x > 0, counting down
np.arange(10, 0, -1)
      array([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])
      
# multiply numpy array by 2
a1 = np.arange(0, 10)
a1 * 2
      array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18])

# add two numpy arrays
a2 = np.arange(10, 20)
a1 + a2
      array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28])
      
# create a 2-dimensional array (2x2)
np.array([[1,2], [3,4]])
      array([[1, 2],
             [3, 4]])
             
# create a 1x20 array, and reshape to a 5x4 2d-array
m = np.arange(0, 20).reshape(5, 4)
      array([[ 0, 1, 2, 3],
            [ 4, 5, 6, 7],
            [ 8, 9, 10, 11],
            [12, 13, 14, 15],
            [16, 17, 18, 19]])
            
# can ask the size along a given axis (0 is rows)
np.size(m, 0)

# and 1 is the columns
np.size(m, 1)

#
#   m[row,column]
#   m[row_start:row_end:row_step , column_start:column_end:column_step]

# select an element in 2d array at row 1 column 2
m[1, 2]

# all items in row 1
m[1,]

# all items in column 2
m[:,2]

It is also possible to select specific rows or columns by passing a Python list as an
element of the slice. The following code explicitly selects by position the first, third,
and fourth rows:
        # using a python array, we can select
        # non-contiguous rows or columns
        m[[1,3,4],:]
        array([[ 4, 5, 6, 7],
               [12, 13, 14, 15],
               [16, 17, 18, 19]])

#################################################################################################
Reshaping arrays
#################################################################################################
# create a 9 element array (1x9)
a = np.arange(0, 9)
# and reshape to a 3x3 2-d array
m = a.reshape(3, 3)

# .ravel will generate array representing a flattened 2-d array
raveled = m.ravel()
                  array([[0, 1, 2],
                         [3, 4, 5],
                         [6, 7, 8]])
                  become
                  array([0, 1, 2, 3, 4, 5, 6, 7, 8])
Even though .reshape() and .ravel() do not change the shape of the original
array or matrix, they do actually return a one-dimensional view into the specified
array or matrix. If you change an element in this view, the value in the original array
or matrix is changed.

The .shape property returns a tuple representing the shape of the array:
# we can reshape by assigning a tuple to the .shape property
# we start with this, which has one dimension
flattened.shape
      (9,)

The property can also be assigned a tuple, which will force the array to reshape itself
as specified:
# and make it 3x3
flattened.shape = (3, 3)
# it is no longer flattened
flattened
          array([[ 0, 1, 2],
                 [ 3, 4, 5],
                 [ 6, 7, 8]])

# transpose a matrix
flattened.transpose()
          array([[ 0, 3, 6],
                 [ 1, 4, 7],
                 [ 2, 5, 8]])

The .resize() method functions similarly to the .reshape() method, except
that while reshaping returns a new array with data copied into it, .resize()
performs an in-place reshaping of the array.


# create a function that is applied to all array elements
a = np.arange(5)
def exp (x):
  return x<3 or x>3

# np.vectorize applies the method to all items in an array
np.vectorize(exp)(a)
      array([ True, True, True, False, True], dtype=bool)
      

# boolean select items < 3
r = a<3
# applying the result of the expression to the [] operator
# selects just the array elements where there is a matching True
a[r]
      array([0, 1, 2])
      

# np.sum treats True as 1 and False as 0
# so this is how many items are less than 3
np.sum(a < 3)
      3
      

# This can be applied across two arrays
a1 = np.arange(0, 5)
a2 = np.arange(5, 0, -1)
a1 < a2
      array([ True, True, True, False, False], dtype=bool)
      
      
# and even multi dimensional arrays
a1 = np.arange(9).reshape(3, 3)
a2 = np.arange(9, 0 , -1).reshape(3, 3)
a1 < a2
      array([[ True, True,True],
            [ True, True, False],
            [False, False, False]], dtype=bool)
            


Horizontal stacking combines two arrays in a manner where the columns of the
second array are placed to the right of those in the first array. The function actually
stacks the two items provided in a two-element tuple. The result is a new array with
data copied from the two that are specified:
# horizontally stack the two arrays
# b becomes columns of a to the right of a's columns
np.hstack((a, b))
        array([[ 0, 1, 2, 10, 20, 30],
               [ 3, 4, 5, 40, 50, 60],
               [ 6, 7, 8, 70, 80, 90]])
               
This functionally is equivalent to using the np.concatenate() function while
specifying axis = 1 :
# identical to concatenate along axis = 1
np.concatenate((a, b), axis = 1)
        array([[ 0, 1, 2, 10, 20, 30],
               [ 3, 4, 5, 40, 50, 60],
               [ 6, 7, 8, 70, 80, 90]])
               
Vertical stacking returns a new array with the contents of the second array as
appended rows of the first array:
In [63]:
# vertical stack, adding b as rows after a's rows
np.vstack((a, b))
          array([[ 0, 1, 2],
                [ 3, 4, 5],
                [ 6, 7, 8],
                [10, 20, 30],
                [40, 50, 60],
                [70, 80, 90]])
                
Like np.hstack() , this is equivalent to using the concatenate function, except
specifying axis=0 :
# concatenate along axis=0 is the same as vstack
np.concatenate((a, b), axis = 0)
        
Depth stacking takes a list of arrays and arranges them in order along an additional
axis referred to as the depth:
In [65]:
# dstack stacks each independent column of a and b
np.dstack((a, b))
          rray([[[ 0, 10],
                [ 1, 20],
                [ 2, 30]],
                [[ 3, 40],
                [ 4, 50],
                [ 5, 60]],
                [[ 6, 70],
                [ 7, 80],
                [ 8, 90]]])        

Column stacking performs a horizontal stack of two one-dimensional arrays, making
each array a column in the resulting array:
one_d_a
      array([0, 1, 2, 3, 4])
one_d_b
      array([10, 20, 30, 40, 50])
# stack the two columns
np.column_stack((one_d_a, one_d_b))
            array([[ 0, 10],
                  [ 1, 20],
                  [ 2, 30],
                  [ 3, 40],
                  [ 4, 50]])

Row stacking returns a new array where each one-dimensional array forms one of
the rows of the new array:
# stack along rows
np.row_stack((one_d_a, one_d_b))
            array([[ 0, 1, 2, 3, 4],
                  [10, 20, 30, 40, 50]])


a
      array([[ 0, 1, 2, 3],
            [ 4, 5, 6, 7],
            [ 8, 9, 10, 11]])
We can split this into four arrays, each representing the values in a specific column:
# horiz split the 2-d array into 4 array columns
np.hsplit(a, 2)
      [array([[0, 1],
              [4, 5],
              [8, 9]]), 
       array([[ 2, 3],
              [ 6, 7],
              [10, 11]])]

The np.split() function performs an identical task when using axis=1 :

Vertical splitting works similarly to horizontal splitting, except against the vertical axis,
np.vsplit(a, 2)
        [array([[0, 1, 2],
               [3, 4, 5]]), 
         array([[ 6, 7, 8],
               [ 9, 10, 11]])]

Depth splitting splits three-dimensional arrays. To demonstrate this, we will use the
following three-dimensional array

The .min() and .max() methods return the minimum and maximum values in an
array. The .argmax() and .argmin() functions return the position of the maximum
or minimum value in the array:
# demonstrate some of the properties of NumPy arrays
m = np.arange(10, 19).reshape(3, 3)
      print ("{0} min of the entire matrix".format(m.min()))
      print ("{0} max of entire matrix".format(m.max()))
      print ("{0} position of the min value".format(m.argmin()))
      print ("{0} position of the max value".format(m.argmax()))
      print ("{0} mins down each column".format(m.min(axis = 0)))
      print ("{0} mins across each row".format(m.min(axis = 1)))
      print ("{0} maxs down each column".format(m.max(axis = 0)))

The .mean() , .std() , and .var() methods compute the mathematical mean,
standard deviation, and variance of the values in an array

The sum and products of all the elements in an array can be computed with the
.sum() and .prod() methods:

The cumulative sum and products can be computed with the .cumsum() and
.cumprod() methods:

The .all() method returns True if all elements of an array are true, and .any()
returns True if any element of the array is true.

The .size property returns the number of elements in the array across all dimensions

Also, .ndim returns the overall dimensionality of an array

###################################################################################
pandas series
###################################################################################
# generate a Series from 5 normal random numbers
np.random.seed(123456)
pd.Series(np.random.randn(5))

# create Series from dict
s6 = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4})
            a 1
            b 2
            c 3
            d 4
# length of the Series
len(s)

# .size is also the # of items in the Series
s.size

The .shape property returns a tuple where the first item is the number of items:
# .shape is a tuple with one value
s.shape

The number of the values that are not part of the NaN can be found by using the .count() method:
# count() returns the number of non-NaN values
s.count()

To determine all of the unique values in a Series , pandas provides the .unique() method:
# all unique values
s.unique()

Also, the count of each of the unique items in a Series can be obtained using
.value_counts() :
# count of non-NaN values, returned max to min order
s.value_counts()
          1 2
          7 1
          6 1
          5 1
          4 1
          3 1
          2 1
          0 1
          

###########################################################################################
Looking up values in Series
###########################################################################################
# single item lookup by index ('a' is the index)
s3['a']

# lookup by position since the index is not an integer
s3[1]

# multiple items
s3[['a', 'c']]

# mean of numpy array values with a NaN
nda = np.array([1, 2, 3, 4, np.NaN])
nda.mean()
        nan
        
When encountering a NaN value, NumPy simply returns NaN . pandas changes this,
so that NaN values are ignored:
# ignores NaN values
s = pd.Series(nda)
s.mean()
      2.5
      
reindex a series:
# change the index
s.index = ['a', 'b', 'c', 'd', 'e']

he following creates a new index for the concatenated result which has
sequential and distinct values.
# reset the index
combined.index = np.arange(0, len(combined))

np.random.seed(123456)
s1 = pd.Series(np.random.randn(4), ['a', 'b', 'c', 'd'])
# reindex with different number of labels
# results in dropped rows and/or NaN's
s2 = s1.reindex(['a', 'c', 'g'])

# fill with 0 instead of NaN
s2 = s.copy()
s2.reindex(['a', 'f'], fill_value=0)
            a -0.173215
            f 0.000000
            
s3 = pd.Series(['red', 'green', 'blue'], index=[0, 3, 5])
s3
        0 red
        3 green
        5 blue
# forward fill example
s3.reindex(np.arange(0,7), method='ffill')
        0 red
        1 red
        2 red
        3 green
        4 green
        5 blue
        6 blue
The following example fills backward using method='bfill' :
# backwards fill example
s3.reindex(np.arange(0,7), method='bfill')
        0 red
        1 green
        2 green
        3 green
        4 blue
        5 blue
        6 NaN


# remove a row / item
del(s['a'])


# equivalent to s.tail(4).head(3)
s[-4:-1]


##############################################################################
The pandas DataFrame Object
##############################################################################
# create a DataFrame from a 2-d ndarray
pd.DataFrame(np.array([[10, 11], [20, 21]]))

# what's the shape of this DataFrame
df1.shape
# it is two rows by 5 columns
          (2,5)

# specify column names
df = pd.DataFrame(np.array([[10, 11], [20, 21]]), columns=['a', 'b'])

# retrieve just the names of the columns by position
"{0}, {1}".format(df.columns[0], df.columns[1])

# rename the columns
df.columns = ['c1', 'c2']

# create a DataFrame with named columns and rows
df = pd.DataFrame(np.array([[0, 1], [2, 3]]),
                  columns=['c1', 'c2'],
                  index=['r1', 'r2'])
                  
# retrieve the index of the DataFrame
df.index

# create a DataFrame with two Series objects
# and a dictionary
s1 = pd.Series(np.arange(1, 6, 1))
s2 = pd.Series(np.arange(6, 11, 1))
pd.DataFrame({'c1': s1, 'c2': s2})
              c1 c2
            0 1 6
            1 2 7
            2 3 8
            3 4 9
            4 5 10
            
# demonstrate alignment during creation
s3 = pd.Series(np.arange(12, 14), index=[1, 2])
df = pd.DataFrame({'c1': s1, 'c2': s2, 'c3': s3})
df
                         c1 c2 c3
                        0 1 6 NaN
                        1 2 7 12
                        2 3 8 13
                        3 4 9 NaN
                        4 5 10 NaN

# read in the data and print the first five rows
# use the Symbol column as the index, and
# only read in columns in positions 0, 2, 3, 7
sp500 = pd.read_csv("data/sp500.csv",
                        index_col='Symbol',
                        usecols=[0, 2, 3, 7])

# get first and second columns (1 and 2) by location
sp500[[1, 2]].head()

# it's a DataFrame, not a Series
type(sp500[[1]].head())
            pandas.core.frame.DataFrame
            
# get price column by name
# result is a Series
sp500['Price']

# get the position of the column with the value of Price
loc = sp500.columns.get_loc('Price')

# by label in both the index and column
sp500.at['MMM', 'Price']
                        141.14
                        
Scalar values can also be looked up by location using .iat by passing both the row
location and then the column location. This is the preferred method of accessing
single values and gives the highest performance.
# by location. Row 0, column 1
sp500.iat[0, 1]

# rename the Book Value column to not have a space
# this returns a copy with the column renamed
df = sp500.rename(columns= {'Book Value': 'BookValue'})
            
# this changes the column in-place
sp500.rename(columns={'Book Value': 'BookValue'}, inplace=True)

# make a copy
copy = sp500.copy()
# add a new column to the copy
copy['TwicePrice'] = sp500.Price * 2

copy = sp500.copy()
# insert sp500.Price * 2 as the
# second column in the DataFrame
copy.insert(1, 'TwicePrice', sp500.Price * 2)

# extract the first four rows and just the Price column
rcopy = sp500[0:3][['Price']].copy()

copy = sp500.copy()
# replace the Price column data with the new values
# instead of adding a new column
copy.Price = sp500.Price * 2

# copy all 500 rows
copy = sp500.copy()
# this just copies the first 2 rows of prices
prices = sp500.iloc[[3, 1, 0]].Price.copy()

# delete the BookValue column
# deletion is in-place
del copy['BookValue']

# this will remove Sector and return it as a series
popped = copy.pop('Sector')
# Sector column removed in-place

# this will return a new DataFrame with 'Sector' removed
# the copy DataFrame is not modified
afterdrop = copy.drop(['Sector'], axis = 1)

# reset the index, moving it into a column
reset_sp500 = sp500.reset_index()

# use read_table with sep=',' to read a CSV
df = pd.read_table("data/msft.csv", sep=',')

# read, but skip rows 0, 2 and 3
df = pd.read_csv("data/msft2.csv", skiprows=[0, 2, 3])

# only process the first three rows
pd.read_csv("data/msft.csv", nrows=3)

# skip 100 lines, then only process the next five
pd.read_csv("data/msft.csv", skiprows=100, nrows=5, header=0, 
            names=['open', 'high', 'low', 'close', 'vol', 'adjclose'])

# write the excel data to a JSON file
df.head().to_json("data/stocks.json")

# read data in from JSON
df_from_json = pd.read_json("data/stocks.json")

# which items are NaN?
df.isnull()
               c1    c2    c3    c4    c5
            a False False False False True
            b False False False True  True
            c False False False True  True
            d False False False True  True
            e False False False True  True
            f False False False False True
            g True True True True True
# which items are not null?
df.notnull()

# .dropna will also return non NaN values
# this gets all non NaN items in column c4
df.c4.dropna()

# on a DataFrame this will drop entire rows where there is at least one NaN
# in this case, that is all rows
df.dropna()

# using how='all', only rows that have all values as NaN will be dropped
df.dropna(how = 'all')


# return a new DataFrame with NaN values filled with 0
filled = df.fillna(0)

# extract the c4 column and fill NaNs forward
df.c4.fillna(method="ffill")

# perform a backwards fill
df.c4.fillna(method="bfill")

# linear interpolate the NaN values from 1 through 2
s = pd.Series([1, np.nan, np.nan, np.nan, 2])
s.interpolate()

# interpolate based upon the values in the index
s.interpolate(method="values")

# reports which rows are duplicates based upon
# if the data in all columns was seen before
data.duplicated()
            0 False
            1 True
            2 False
            3 False

# drop duplicate rows retaining first row of the duplicates
data.drop_duplicates()

# demonstrate applying a function to every item of a Series
s = pd.Series(np.arange(0, 5))
s.apply(lambda v: v * 2)

# group by both sensor and axis values
mcg = sensors.groupby(['sensor', 'axis'])
print_groups(mcg)

pandas allows the application of an aggregation function to each group of data.
Aggregation is performed using the .aggregate() (or in short, .agg()) method
of the GroupBy object. The parameter of .agg() is a reference to a function that is
applied to each group. In the case of DataFrame, the function will be applied to
each column.


# do not create an index matching the original object
sensors.groupby(['sensor', 'axis'],
as_index=False).agg(np.mean)




###############################################################################
pd.concat() vs pd.append()
###############################################################################
pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,
       keys=None, levels=None, names=None, verify_integrity=False)

    objs: a sequence or mapping of Series, DataFrame, or Panel objects. If a 
          dict is passed, the sorted keys will be used as the keys argument, 
          unless it is passed, in which case the values will be selected 
          (see below). Any None objects will be dropped silently unless they 
          are all None in which case a ValueError will be raised.
    axis: {0, 1, ...}, default 0. The axis to concatenate along.
    join: {‘inner’, ‘outer’}, default ‘outer’. How to handle indexes on other 
          axis(es). Outer for union and inner for intersection.
    join_axes: list of Index objects. Specific indexes to use for the other 
               n - 1 axes instead of performing inner/outer set logic.
    keys: sequence, default None. Construct hierarchical index using the passed 
          keys as the outermost level. If multiple levels passed, should contain 
          tuples.
    levels : list of sequences, default None. Specific levels (unique values) 
             to use for constructing a MultiIndex. Otherwise they will be inferred 
             from the keys.
    names: list, default None. Names for the levels in the resulting hierarchical index.
    verify_integrity: boolean, default False. Check whether the new concatenated 
                      axis contains duplicates. This can be very expensive relative 
                      to the actual data concatenation.
    ignore_index : boolean, default False. If True, do not use the index values 
                   on the concatenation axis. The resulting axis will be labeled 
                   0, ..., n - 1. This is useful if you are concatenating objects 
                   where the concatenation axis does not have meaningful indexing 
                   information. Note the index values on the other axes are still 
                   respected in the join.
    copy : boolean, default True. If False, do not copy data unnecessarily.


merge(left, right, how='inner', on=None, left_on=None, right_on=None,
      left_index=False, right_index=False, sort=True,
      suffixes=('_x', '_y'), copy=True, indicator=False)

Here’s a description of what each argument is for:

        left: A DataFrame object

        right: Another DataFrame object

        on: Columns (names) to join on. Must be found in both the left and right 
            DataFrame objects. If not passed and left_index and right_index are 
            False, the intersection of the columns in the DataFrames will be 
            inferred to be the join keys

        left_on: Columns from the left DataFrame to use as keys. Can either be 
                 column names or arrays with length equal to the length of the DataFrame

        right_on: Columns from the right DataFrame to use as keys. Can either 
                  be column names or arrays with length equal to the length of the DataFrame

        left_index: If True, use the index (row labels) from the left DataFrame as 
                    its join key(s). In the case of a DataFrame with a MultiIndex 
                    (hierarchical), the number of levels must match the number of 
                    join keys from the right DataFrame

        right_index: Same usage as left_index for the right DataFrame

        how: One of 'left', 'right', 'outer', 'inner'. Defaults to inner. See below 
             for more detailed description of each method

        sort: Sort the result DataFrame by the join keys in lexicographical order. 
              Defaults to True, setting to False will improve performance substantially 
              in many cases

        suffixes: A tuple of string suffixes to apply to overlapping columns. Defaults to ('_x', '_y').

        copy: Always copy data (default True) from the passed DataFrame objects, even 
              when reindexing is not necessary. Cannot be avoided in many cases but may 
              improve performance / memory usage. The cases where copying can be avoided 
              are somewhat pathological but this option is provided nonetheless.

        indicator: Add a column to the output DataFrame called _merge with information 
                   on the source of each row. _merge is Categorical-type and takes on a 
                   value of left_only for observations whose merge key only appears in 
                   'left' DataFrame, right_only for observations whose merge key only 
                   appears in 'right' DataFrame, and both if the observation’s merge 
                   key is found in both.
                   
Joining on index

DataFrame.join is a convenient method for combining the columns of two potentially 
differently-indexed DataFrames into a single result DataFrame. 

